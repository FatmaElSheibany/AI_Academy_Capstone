{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac4a5cda",
   "metadata": {},
   "source": [
    "# This notebook contains all the data preparation/cleaning steps for our highest score submission\n",
    "\n",
    "it takes in as input \n",
    "    - all the 33 well datasets given as part of the project, \n",
    "    - plus the label_code file\n",
    "    \n",
    "(and after running all the cells), it outputs the prepared dataset used for training a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0e596368",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dbbdb0a",
   "metadata": {},
   "source": [
    "# Import all Datasets \n",
    "Total number of datasets = 33\n",
    "\n",
    "Total number of wells = 19\n",
    "\n",
    "12 wells have two datasets each, example: well_15_9-13_ = 15_9-13_part_1 & 15_9-13_part_2); \n",
    "\n",
    "The remaining 9 wells only contain one dataset each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b2f49828",
   "metadata": {},
   "outputs": [],
   "source": [
    "_15_9_13_part_1 = pd.read_csv(r\"C:\\Users\\fal-s\\Desktop\\data science\\AI Academy\\capstone project\\csv files\\15_9-13_part_1.csv\")\n",
    "_15_9_13_part_2 = pd.read_csv(r'C:\\Users\\fal-s\\Desktop\\data science\\AI Academy\\capstone project\\csv files\\15_9-13_part_2.csv')\n",
    "#---------------------------------------------------------------------------------------------------------------\n",
    "_15_9_17_part_1 = pd.read_csv(r'C:\\Users\\fal-s\\Desktop\\data science\\AI Academy\\capstone project\\csv files\\15_9-17_part_1.csv')\n",
    "_15_9_17_part_2 = pd.read_csv(r'C:\\Users\\fal-s\\Desktop\\data science\\AI Academy\\capstone project\\csv files\\15_9-17_part_2.csv')\n",
    "#---------------------------------------------------------------------------------------------------------------\n",
    "_16_1_2_part_2 = pd.read_csv(r'C:\\Users\\fal-s\\Desktop\\data science\\AI Academy\\capstone project\\csv files\\16_1-2_part_2.csv')\n",
    "#---------------------------------------------------------------------------------------------------------------\n",
    "_16_2_6_part_2 = pd.read_csv(r'C:\\Users\\fal-s\\Desktop\\data science\\AI Academy\\capstone project\\csv files\\16_2-6_part_2.csv')\n",
    "#---------------------------------------------------------------------------------------------------------------\n",
    "_16_2_7_part_1 = pd.read_csv(r'C:\\Users\\fal-s\\Desktop\\data science\\AI Academy\\capstone project\\csv files\\16_2-7_part_1.csv')\n",
    "_16_2_7_part_2 = pd.read_csv(r'C:\\Users\\fal-s\\Desktop\\data science\\AI Academy\\capstone project\\csv files\\16_2-7_part_2.csv')\n",
    "#---------------------------------------------------------------------------------------------------------------\n",
    "_16_2_16_part_1 = pd.read_csv(r'C:\\Users\\fal-s\\Desktop\\data science\\AI Academy\\capstone project\\csv files\\16_2-16_part_1.csv')\n",
    "_16_2_16_part_2 = pd.read_csv(r'C:\\Users\\fal-s\\Desktop\\data science\\AI Academy\\capstone project\\csv files\\16_2-16_part_2.csv')\n",
    "#---------------------------------------------------------------------------------------------------------------\n",
    "_16_4_1_part_1 = pd.read_csv(r'C:\\Users\\fal-s\\Desktop\\data science\\AI Academy\\capstone project\\csv files\\16_4-1_part_1.csv')\n",
    "_16_4_1_part_2 = pd.read_csv(r'C:\\Users\\fal-s\\Desktop\\data science\\AI Academy\\capstone project\\csv files\\16_4-1_part_2.csv')\n",
    "#---------------------------------------------------------------------------------------------------------------\n",
    "_16_7_6_part_1 = pd.read_csv(r'C:\\Users\\fal-s\\Desktop\\data science\\AI Academy\\capstone project\\csv files\\16_7-6_part_1.csv')\n",
    "#---------------------------------------------------------------------------------------------------------------\n",
    "_16_8_1_part_2 = pd.read_csv(r'C:\\Users\\fal-s\\Desktop\\data science\\AI Academy\\capstone project\\csv files\\16_8-1_part_2.csv')\n",
    "#---------------------------------------------------------------------------------------------------------------\n",
    "_16_10_1_part_1 = pd.read_csv(r'C:\\Users\\fal-s\\Desktop\\data science\\AI Academy\\capstone project\\csv files\\16_10-1_part_1.csv')\n",
    "_16_10_1_part_2 = pd.read_csv(r'C:\\Users\\fal-s\\Desktop\\data science\\AI Academy\\capstone project\\csv files\\16_10-1_part_2.csv')\n",
    "#---------------------------------------------------------------------------------------------------------------\n",
    "_17_4_1_part_1 = pd.read_csv(r'C:\\Users\\fal-s\\Desktop\\data science\\AI Academy\\capstone project\\csv files\\17_4-1_part_1.csv')\n",
    "#---------------------------------------------------------------------------------------------------------------\n",
    "_17_11_1_part_1 = pd.read_csv(r'C:\\Users\\fal-s\\Desktop\\data science\\AI Academy\\capstone project\\csv files\\17_11-1_part_1.csv')\n",
    "_17_11_1_part_2 = pd.read_csv(r'C:\\Users\\fal-s\\Desktop\\data science\\AI Academy\\capstone project\\csv files\\17_11-1_part_2.csv')\n",
    "#---------------------------------------------------------------------------------------------------------------\n",
    "_25_2_7_part_1 = pd.read_csv(r'C:\\Users\\fal-s\\Desktop\\data science\\AI Academy\\capstone project\\csv files\\25_2-7_part_1.csv')\n",
    "#---------------------------------------------------------------------------------------------------------------\n",
    "_25_3_1_part_1 = pd.read_csv(r'C:\\Users\\fal-s\\Desktop\\data science\\AI Academy\\capstone project\\csv files\\25_3-1_part_1.csv')\n",
    "_25_3_1_part_2 = pd.read_csv(r'C:\\Users\\fal-s\\Desktop\\data science\\AI Academy\\capstone project\\csv files\\25_3-1_part_2.csv')\n",
    "#---------------------------------------------------------------------------------------------------------------\n",
    "_25_5_1_part_1 = pd.read_csv(r'C:\\Users\\fal-s\\Desktop\\data science\\AI Academy\\capstone project\\csv files\\25_5-1_part_1.csv')\n",
    "_25_5_1_part_2 = pd.read_csv(r'C:\\Users\\fal-s\\Desktop\\data science\\AI Academy\\capstone project\\csv files\\25_5-1_part_2.csv')\n",
    "#---------------------------------------------------------------------------------------------------------------\n",
    "_25_8_7_part_1 = pd.read_csv(r'C:\\Users\\fal-s\\Desktop\\data science\\AI Academy\\capstone project\\csv files\\25_8-7_part_1.csv')\n",
    "#---------------------------------------------------------------------------------------------------------------\n",
    "_25_9_1_part_1 = pd.read_csv(r'C:\\Users\\fal-s\\Desktop\\data science\\AI Academy\\capstone project\\csv files\\25_9-1_part_1.csv')\n",
    "_25_9_1_part_2 = pd.read_csv(r'C:\\Users\\fal-s\\Desktop\\data science\\AI Academy\\capstone project\\csv files\\25_9-1_part_2.csv')\n",
    "#---------------------------------------------------------------------------------------------------------------\n",
    "_29_3_1_part_1 = pd.read_csv(r'C:\\Users\\fal-s\\Desktop\\data science\\AI Academy\\capstone project\\csv files\\29_3-1_part_1.csv')\n",
    "_29_3_1_part_2 = pd.read_csv(r'C:\\Users\\fal-s\\Desktop\\data science\\AI Academy\\capstone project\\csv files\\29_3-1_part_2.csv')\n",
    "#---------------------------------------------------------------------------------------------------------------\n",
    "_30_3_3_part_1 = pd.read_csv(r'C:\\Users\\fal-s\\Desktop\\data science\\AI Academy\\capstone project\\csv files\\30_3-3_part_1.csv')\n",
    "_30_3_3_part_2 = pd.read_csv(r'C:\\Users\\fal-s\\Desktop\\data science\\AI Academy\\capstone project\\csv files\\30_3-3_part_2.csv')\n",
    "#---------------------------------------------------------------------------------------------------------------\n",
    "Labels_code = pd.read_csv(r'C:\\Users\\fal-s\\Desktop\\data science\\AI Academy\\capstone project\\csv files\\Labels_code.csv')\n",
    "Test_dataset = pd.read_csv(r'C:\\Users\\fal-s\\Desktop\\data science\\AI Academy\\capstone project\\csv files\\Test_dataset.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da8b0104",
   "metadata": {},
   "source": [
    "# Dataset Analysis\n",
    "Check that all datasets contain similar columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c92c1d46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All DataFrames contain same set of columns:\n",
      "Index(['file_name', 'filename', 'Lithology_code', 'ROPA', 'BS', 'PEF', 'RXO',\n",
      "       'DCAL', 'RMIC', 'ROP', 'SP', 'RMED', 'GR', 'DTC', 'MUDWEIGHT', 'Y_LOC',\n",
      "       'DRHO', 'DEPTH_MD', 'DTS', 'RHOB', 'NPHI', 'SGR', 'X_LOC', 'CALI',\n",
      "       'RSHA', 'Z_LOC', 'RDEP', 'DEPT'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Define a list of containing all dataset names\n",
    "df_names = [\n",
    "    \"_15_9_13_part_1\", \"_15_9_13_part_2\", \"_15_9_17_part_1\", \"_15_9_17_part_2\",\n",
    "    \"_16_2_7_part_1\", \"_16_2_7_part_2\", \"_16_2_16_part_1\", \"_16_2_16_part_2\",\n",
    "    \"_16_4_1_part_1\", \"_16_4_1_part_2\", \"_16_10_1_part_1\", \"_16_10_1_part_2\",\n",
    "    \"_17_11_1_part_1\", \"_17_11_1_part_2\", \"_25_3_1_part_1\", \"_25_3_1_part_2\",\n",
    "    \"_25_5_1_part_1\", \"_25_5_1_part_2\", \"_25_9_1_part_1\", \"_25_9_1_part_2\",\n",
    "    \"_29_3_1_part_1\", \"_29_3_1_part_2\", \"_30_3_3_part_1\", \"_30_3_3_part_2\",\n",
    "    \"_16_1_2_part_2\", \"_16_2_6_part_2\", \"_16_7_6_part_1\", \"_16_8_1_part_2\",\n",
    "    \"_17_4_1_part_1\", \"_25_2_7_part_1\", \"_25_8_7_part_1\"\n",
    "]\n",
    "\n",
    "# Create an empty list to store DataFrames\n",
    "dataframes = []\n",
    "\n",
    "# Assuming you have DataFrames stored in variables with names matching df_names\n",
    "# Add your DataFrames to the list\n",
    "for df_name in df_names:\n",
    "    dataframes.append(globals()[df_name])\n",
    "    \n",
    "similar_columns = dataframes[0].columns\n",
    "match = True\n",
    "\n",
    "for df in dataframes:\n",
    "    mask1 = np.all(np.in1d(df.columns, similar_columns)) #all columns in df are in array similar_columns\n",
    "    mask2 = np.all(np.in1d(similar_columns, df.columns))#all columns in array similar_columns are in df\n",
    "    \n",
    "    if ~(mask1 & mask2):  #if cols for at least 2 datasets are different\n",
    "        match = False\n",
    "\n",
    "if match == True: \n",
    "    print(\"All DataFrames contain same set of columns:\")\n",
    "    print(similar_columns)\n",
    "else: \n",
    "    print(\"DataFrames do not contain same set of columns.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c949a17",
   "metadata": {},
   "source": [
    "# Dataset Preparation\n",
    "we'll now process each dataset to \n",
    "\n",
    "1)\tDrop the column [“filename”] from the dataset since it’s entirely empty. \n",
    "\n",
    "2)\tRemove rows with empty cells in [“Lithology_code”] since the aim of the project is to predict the lithology. \n",
    "\n",
    "3)\tExtract numerical value from column [“DTC”] using regex pattern "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e9fbf9",
   "metadata": {},
   "source": [
    "# <span style=\"color: red;\">Preparing each data separately</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b6339761",
   "metadata": {},
   "outputs": [],
   "source": [
    "#list of dfs to be clipped with exceptions\n",
    "df_names_prep2 = [\"_16_2_6_part_2\", \"_25_3_1_part_2\", \"_15_9_13_part_2\", \"_16_10_1_part_1\", \"_16_10_1_part_2\", \"_25_9_1_part_2\",\n",
    "             \"_16_2_16_part_2\", \"_17_11_1_part_2\", \"_30_3_3_part_1\", \"_30_3_3_part_2\", \"_15_9_17_part_2\", \"_16_2_7_part_1\"]\n",
    "\n",
    "#list of dfs to be clipped using 5*IQR as the boundary setting\n",
    "df_names_prep1 = np.setdiff1d(df_names, df_names_prep2) #list of all wells in df_names but not in dfs_prep2\n",
    "\n",
    "# Creating lists of the dfs\n",
    "dfs_prep1 = []\n",
    "dfs_prep2 = []\n",
    "\n",
    "for df_name in df_names_prep1:\n",
    "    dfs_prep1.append(globals()[df_name])\n",
    "    \n",
    "for df_name in df_names_prep2:\n",
    "    dfs_prep1.append(globals()[df_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "92c54d6c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fal-s\\AppData\\Local\\Temp\\ipykernel_2248\\2080575646.py:29: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.loc[:, feature] = df.loc[:, feature].clip(lower=lower, upper=upper)\n",
      "C:\\Users\\fal-s\\AppData\\Local\\Temp\\ipykernel_2248\\2080575646.py:29: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.loc[:, feature] = df.loc[:, feature].clip(lower=lower, upper=upper)\n",
      "C:\\Users\\fal-s\\AppData\\Local\\Temp\\ipykernel_2248\\2080575646.py:29: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.loc[:, feature] = df.loc[:, feature].clip(lower=lower, upper=upper)\n",
      "C:\\Users\\fal-s\\AppData\\Local\\Temp\\ipykernel_2248\\2080575646.py:29: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.loc[:, feature] = df.loc[:, feature].clip(lower=lower, upper=upper)\n",
      "C:\\Users\\fal-s\\AppData\\Local\\Temp\\ipykernel_2248\\2080575646.py:29: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.loc[:, feature] = df.loc[:, feature].clip(lower=lower, upper=upper)\n",
      "C:\\Users\\fal-s\\AppData\\Local\\Temp\\ipykernel_2248\\2080575646.py:29: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.loc[:, feature] = df.loc[:, feature].clip(lower=lower, upper=upper)\n",
      "C:\\Users\\fal-s\\AppData\\Local\\Temp\\ipykernel_2248\\2080575646.py:29: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.loc[:, feature] = df.loc[:, feature].clip(lower=lower, upper=upper)\n",
      "C:\\Users\\fal-s\\AppData\\Local\\Temp\\ipykernel_2248\\2080575646.py:29: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.loc[:, feature] = df.loc[:, feature].clip(lower=lower, upper=upper)\n"
     ]
    }
   ],
   "source": [
    "for i, df in enumerate(dataframes): \n",
    "    \n",
    "#     print(f\"processing df at index {i}\")\n",
    "    \n",
    "    df.drop('filename', axis = 1, inplace = True) #dropping the 'filename' column\n",
    "    df.dropna(subset=['Lithology_code'], inplace = True) #drop unlabelled samples\n",
    "        \n",
    "    if df['DTC'].dtype == 'object':\n",
    "        df['DTC'] = df['DTC'].str.extract('([-+]?(?:[0-9]+(?:\\.[0-9]*)?|\\.[0-9]+))') #extract numerical value from DTC col\n",
    "    \n",
    "    df.fillna(method = 'ffill', inplace = True)\n",
    "    df.fillna(method = 'bfill', inplace = True)\n",
    "\n",
    "    if df in dfs_prep2: #these dfs were clipped using different boundaries so skip the clipping code below; \n",
    "        #they'll be processed individually later\n",
    "        continue\n",
    "        \n",
    "    for feature in ['PEF','RMED', 'GR']:\n",
    "        mask = df[feature].notnull()\n",
    "        \n",
    "        if df[feature].dtype in (int, float):\n",
    "            Q1 = df.loc[mask, feature].quantile(0.25) #first quartile\n",
    "            Q3 = df.loc[mask, feature].quantile(0.75) #third quartile\n",
    "\n",
    "            IQR = Q3 - Q1\n",
    "            lower = Q1 - 5*IQR #lower bound of clipping\n",
    "            upper = Q3 + 5*IQR #upper bound of clipping\n",
    "\n",
    "            df.loc[:, feature] = df.loc[:, feature].clip(lower=lower, upper=upper)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ad6b2438",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip_outliers(df, feature, fac): \n",
    "    if df[feature].dtype in (int, float):\n",
    "        Q1 = df.loc[df[feature].notnull(), feature].quantile(0.25) #first quartile\n",
    "        Q3 = df.loc[df[feature].notnull(), feature].quantile(0.75) #third quartile\n",
    "\n",
    "        IQR = Q3 - Q1\n",
    "        \n",
    "        lower = Q1 - fac*IQR #lower bound of clipping\n",
    "        upper = Q3 + fac*IQR #upper bound of clipping\n",
    "\n",
    "        df[feature] = df[feature].clip(lower=lower, upper=upper)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2a899091",
   "metadata": {},
   "outputs": [],
   "source": [
    "#well 16_2_6_part_2 \n",
    "_16_2_6_part_2 = clip_outliers(_16_2_6_part_2, 'PEF', 1.5)\n",
    "_16_2_6_part_2 = clip_outliers(_16_2_6_part_2, 'GR', 5)\n",
    "\n",
    "#well 25_3_1_part_2 \n",
    "_25_3_1_part_2 = clip_outliers(_25_3_1_part_2, 'PEF', 1.5)\n",
    "_25_3_1_part_2 = clip_outliers(_25_3_1_part_2, 'GR', 5)\n",
    "\n",
    "#well 15_9_13_part_2 \n",
    "_15_9_13_part_2 = clip_outliers(_15_9_13_part_2, 'GR', 5)\n",
    "_15_9_13_part_2 = clip_outliers(_15_9_13_part_2, 'PEF', 5)\n",
    "_15_9_13_part_2['PEF'] = _15_9_13_part_2['PEF'].clip(lower=min(_15_9_13_part_2['PEF']), upper=7.775)\n",
    "\n",
    "#well 16_10_1_part_1 \n",
    "_16_10_1_part_1 = clip_outliers(_16_10_1_part_1, 'GR', 5)\n",
    "_16_10_1_part_1 = clip_outliers(_16_10_1_part_1, 'PEF', 1.5)\n",
    "\n",
    "#well 16_10_1_part_2 \n",
    "_16_10_1_part_2 = clip_outliers(_16_10_1_part_2, 'RMED', 5)\n",
    "_16_10_1_part_2 = clip_outliers(_16_10_1_part_2, 'GR', 5)\n",
    "_16_10_1_part_2 = clip_outliers(_16_10_1_part_2, 'PEF', 1.5)\n",
    "\n",
    "#well 25_9_1_part_2 \n",
    "_25_9_1_part_2 = clip_outliers(_25_9_1_part_2, 'RMED', 1.5)\n",
    "_25_9_1_part_2 = clip_outliers(_25_9_1_part_2, 'GR', 5)\n",
    "\n",
    "#well 16_2_16_part_2 \n",
    "_16_2_16_part_2  = clip_outliers(_16_2_16_part_2 , 'RMED', 5)\n",
    "_16_2_16_part_2  = clip_outliers(_16_2_16_part_2 , 'GR', 5)\n",
    "_16_2_16_part_2  = clip_outliers(_16_2_16_part_2 , 'PEF', 5)\n",
    "_16_2_16_part_2 ['PEF'] = _16_2_16_part_2 ['PEF'].clip(lower=min(_16_2_16_part_2 ['PEF']), upper=10)\n",
    "\n",
    "#well 17_11_1_part_2 \n",
    "_17_11_1_part_2   = clip_outliers(_17_11_1_part_2, 'GR', 5)\n",
    "\n",
    "#well 30_3_3_part_1\n",
    "_30_3_3_part_1   = clip_outliers(_30_3_3_part_1 , 'RMED', 1.5)\n",
    "_30_3_3_part_1   = clip_outliers(_30_3_3_part_1 , 'GR', 5)\n",
    "_30_3_3_part_1   = clip_outliers(_30_3_3_part_1 , 'PEF', 1.5)\n",
    "\n",
    "#well 30_3_3_part_2\n",
    "_30_3_3_part_2   = clip_outliers(_30_3_3_part_2 , 'RMED', 1.5)\n",
    "_30_3_3_part_2   = clip_outliers(_30_3_3_part_2 , 'GR', 5)\n",
    "_30_3_3_part_2   = clip_outliers(_30_3_3_part_2 , 'PEF', 1.5)\n",
    "\n",
    "#well 15_9_17_part_2\n",
    "_15_9_17_part_2  = clip_outliers(_15_9_17_part_2 , 'RMED', 1.5)\n",
    "_15_9_17_part_2  = clip_outliers(_15_9_17_part_2 , 'GR', 5)\n",
    "_15_9_17_part_2  = clip_outliers(_15_9_17_part_2 , 'PEF', 1.5)\n",
    "\n",
    "#well 16_2_7_part_1\n",
    "_16_2_7_part_1  = clip_outliers(_16_2_7_part_1 , 'GR', 5)\n",
    "_16_2_7_part_1 = clip_outliers(_16_2_7_part_1 , 'PEF', 1.5)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24baaf7e",
   "metadata": {},
   "source": [
    "# preparing the training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2dc66882",
   "metadata": {},
   "outputs": [],
   "source": [
    "#concatenating all the datasets together\n",
    "df = pd.concat ([_15_9_13_part_1,_15_9_13_part_2, _15_9_17_part_1,_15_9_17_part_2, _16_2_7_part_1,_16_2_7_part_2,\n",
    "      _16_2_16_part_1,_16_2_16_part_2, _16_4_1_part_1,_16_4_1_part_2, _16_10_1_part_1,_16_10_1_part_2,\n",
    "      _17_11_1_part_1,_17_11_1_part_2, _25_3_1_part_1,_25_3_1_part_2, _25_5_1_part_1,_25_5_1_part_2,\n",
    "      _25_9_1_part_2, _29_3_1_part_1,_29_3_1_part_2, _30_3_3_part_1,_30_3_3_part_2, \n",
    "     _16_1_2_part_2, _16_2_6_part_2, _16_7_6_part_1, _16_8_1_part_2, _17_4_1_part_1, _25_2_7_part_1, _25_8_7_part_1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5e216d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['RMIC', 'RXO', 'DCAL', 'BS', 'ROPA', 'DTS', 'SGR', 'RDEP'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "40fee28a",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['PEF', 'GR', 'RMED', 'DRHO', 'RSHA', 'ROP']\n",
    "\n",
    "for feature in features: \n",
    "    df = clip_outliers(df,feature,1.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "97e2527c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label_code</th>\n",
       "      <th>label_name</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>30000</td>\n",
       "      <td>Sandstone</td>\n",
       "      <td>non shaly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>65030</td>\n",
       "      <td>Sandstone/Shale</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>65000</td>\n",
       "      <td>Shale</td>\n",
       "      <td>shaly (clay rich)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>80000</td>\n",
       "      <td>Marl</td>\n",
       "      <td>shaly (clay rich)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>74000</td>\n",
       "      <td>Dolomite</td>\n",
       "      <td>non shaly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>70000</td>\n",
       "      <td>Limestone</td>\n",
       "      <td>non shaly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>70032</td>\n",
       "      <td>Chalk</td>\n",
       "      <td>non shaly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>88000</td>\n",
       "      <td>Halite</td>\n",
       "      <td>non shaly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>86000</td>\n",
       "      <td>Anhydrite</td>\n",
       "      <td>non shaly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>99000</td>\n",
       "      <td>Tuff</td>\n",
       "      <td>non shaly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>90000</td>\n",
       "      <td>Coal</td>\n",
       "      <td>non shaly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>93000</td>\n",
       "      <td>Basement</td>\n",
       "      <td>non shaly</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    label_code       label_name           category\n",
       "0        30000        Sandstone          non shaly\n",
       "1        65030  Sandstone/Shale                  -\n",
       "2        65000            Shale  shaly (clay rich)\n",
       "3        80000             Marl  shaly (clay rich)\n",
       "4        74000         Dolomite          non shaly\n",
       "5        70000        Limestone          non shaly\n",
       "6        70032            Chalk          non shaly\n",
       "7        88000           Halite          non shaly\n",
       "8        86000        Anhydrite          non shaly\n",
       "9        99000             Tuff          non shaly\n",
       "10       90000             Coal          non shaly\n",
       "11       93000         Basement          non shaly"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Labels_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5038559e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['label_name'] = df['Lithology_code'].map(Labels_code.set_index('label_code')['label_name'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "59d641c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop('Lithology_code', axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "45b80971",
   "metadata": {},
   "outputs": [],
   "source": [
    "#rearranging the cols\n",
    "\n",
    "df = df[['file_name', 'label_name', 'PEF', 'ROP', 'SP', 'RMED', 'GR', 'DTC', 'MUDWEIGHT',\n",
    "       'Y_LOC', 'DRHO', 'DEPTH_MD', 'RHOB', 'NPHI', 'X_LOC', 'CALI', 'RSHA',\n",
    "       'Z_LOC', 'DEPT']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "3ec9ac25",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.reset_index(drop = True, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d5cf59",
   "metadata": {},
   "source": [
    "# end of preparation\n",
    "\n",
    "# you can export df and/or use it for modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec556d81",
   "metadata": {},
   "source": [
    "# comparing the output of this notebook with the prepared dataset in dataiku\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "95f5f953",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataiku_file = pd.read_csv(r\"C:\\Users\\fal-s\\Downloads\\Fully_stacked_prepared_sub39.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d78243cd",
   "metadata": {},
   "source": [
    "### confirming columns match "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "0d9bafb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Both DataFrames contain same set of columns:\n",
      "Index(['file_name', 'filename', 'Lithology_code', 'ROPA', 'BS', 'PEF', 'RXO',\n",
      "       'DCAL', 'RMIC', 'ROP', 'SP', 'RMED', 'GR', 'DTC', 'MUDWEIGHT', 'Y_LOC',\n",
      "       'DRHO', 'DEPTH_MD', 'DTS', 'RHOB', 'NPHI', 'SGR', 'X_LOC', 'CALI',\n",
      "       'RSHA', 'Z_LOC', 'RDEP', 'DEPT'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "mask1 = np.all(np.in1d(df.columns, dataiku_file.columns)) \n",
    "mask2 = np.all(np.in1d(dataiku_file.columns, df.columns))\n",
    "\n",
    "if ~(mask1 & mask2):  #if cols of the 2 datasets are different\n",
    "    match = False\n",
    "\n",
    "if match == True: \n",
    "    print(\"Both DataFrames contain same set of columns:\")\n",
    "    print(similar_columns)\n",
    "else: \n",
    "    print(\"The DataFrames do not contain same set of columns.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f14a610",
   "metadata": {},
   "source": [
    "### confirming values match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "243f2fba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{True}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(df.columns == dataiku_file.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "3d743788",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{True}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(df.columns == dataiku_file.columns)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
